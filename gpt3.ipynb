{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key='your-key-here')\n",
    "\n",
    "# Load prompts from a JSONL file\n",
    "def load_prompts(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "op_prompts = load_prompts('op_test_gpt.jsonl')\n",
    "pairs_prompts = load_prompts('pairs_test_gpt.jsonl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T03:00:17.715942Z",
     "start_time": "2024-10-01T03:00:17.636987Z"
    }
   },
   "id": "f02ac414ef772fff",
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to send requests and gather responses\n",
    "def get_responses(prompts, instruction, desc=\"Getting GPT3.5 Responses: \"):\n",
    "    responses = []\n",
    "    for prompt in tqdm(prompts, desc=desc):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=[\n",
    "                          {\"role\":\"system\", \"content\":instruction},\n",
    "                          prompt['body']['messages'][1]\n",
    "                      ],\n",
    "                    max_completion_tokens=prompt['body']['max_tokens'],\n",
    "                    )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {prompt['custom_id']}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T03:00:18.739159Z",
     "start_time": "2024-10-01T03:00:18.732459Z"
    }
   },
   "id": "69a03815dbdf6e2a",
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# Get responses of Original Posts from GPT3.5\n",
    "directly_predict = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? Answer with 'malleable' or 'resistant'.\"\n",
    "predict_then_explain = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? Answer with 'malleable' or 'resistant' and explain your answer. Response with the following format: Prediction: resistant/malleable \\n Explanation: briefly explain here.\"\n",
    "explain_then_predict = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? First briefly explain your analysis and then give your answer with resistant/malleable. Response with the following format: Explanation: briefly explain here. \\n  Prediction: resistant/malleable\"\n",
    "\n",
    "op_responses_direct = get_responses(op_prompts, directly_predict)\n",
    "op_responses_pred_explain = get_responses(op_prompts, predict_then_explain)\n",
    "op_responses_explain_pred = get_responses(op_prompts, explain_then_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:07:02.542363Z",
     "start_time": "2024-09-29T16:56:24.373934Z"
    }
   },
   "id": "b85ddc576407e332"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T03:21:25.958283Z",
     "start_time": "2024-10-01T03:19:59.334260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get responses of Pairs(op, reply1, reply2) from GPT3.5\n",
    "directly_predict = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now judge which response succeeded in persuading. Answer only with first/second.\"\n",
    "predict_then_explain = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now first judge which response succeeded in persuading and then explain your analysis very briefly. Response with the following format: Prediction: answer only with first or second \\n Explanation: briefly explain here.\"\n",
    "explain_then_predict = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now first show your analysis very briefly and then judge which response succeeded in persuading. Response with the following format: Explanation: briefly analyse here. \\n Prediction: answer only with first or second\"\n",
    "\n",
    "knowledge = \"Hint: 1. Language Dissimilarity with Original Post: Persuasive replies use different content words but match in stopwords 2.Reply Length: Longer replies tend to be more persuasive, as they can convey more information and elaborate on points effectively. 3. Language Dissimilarity with Original Post: Persuasive replies use different content words but match in stopwords. 4. Links and Evidence: Including links as evidence in an argument increases the chances of persuasion. 5. Calmer Tone: Replies that use calmer, less intense language are more likely to persuade, as they come across as more composed. 6. Positive Emotion and Sentiment: Persuasive replies include a mix of positive and negative sentiment.\"\n",
    "\n",
    "pairs_responses_direct = get_responses(pairs_prompts, directly_predict)\n",
    "pairs_responses_pred_explain = get_responses(pairs_prompts, predict_then_explain)\n",
    "pairs_responses_explain_pred = get_responses(pairs_prompts, explain_then_predict)\n",
    "pairs_responses_with_knowledge = get_responses(pairs_prompts, directly_predict + knowledge)"
   ],
   "id": "10629898ab7633f4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting GPT3.5 Responses: 100%|██████████| 200/200 [01:26<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "instruction_1 = f\"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. Analyze the persuasiveness of the two replies respectively in two paragraphs. Note only analysis, not conclusion. \\n {knowledge}\"\n",
    "# Function to send requests and gather responses\n",
    "def get_two_stage_responses(prompts, instruction_1, instruction_2):\n",
    "    stage_one_responses = get_responses(prompts, instruction_1, desc=\"Getting GPT3.5 Responses(1st stage): \")\n",
    "    stage_one_responses = [res.choices[0].message.content for res in stage_one_responses]\n",
    "    stage_two_responses = []\n",
    "    for prompt in tqdm(stage_one_responses, desc=\"Getting GPT3.5 Responses(2nd stage): \"):\n",
    "        response = client.chat.completions.create(\n",
    "                  model=\"gpt-3.5-turbo\",\n",
    "                  messages=[\n",
    "                      {\"role\":\"system\", \"content\":instruction_2},\n",
    "                      {\"role\": \"user\", \"content\":prompt},\n",
    "                  ],\n",
    "                max_completion_tokens=1000,\n",
    "                )\n",
    "        stage_two_responses.append(response)\n",
    "    return stage_two_responses"
   ],
   "id": "a488f109a070fbfc"
  },
  {
   "cell_type": "code",
   "source": [
    "def prediction_accuracy(responses, file_path, method=\"direct\"):\n",
    "    gpt_predictions = []\n",
    "    if method == \"direct\":\n",
    "        for res in responses:\n",
    "            gpt_predictions.append(res.choices[0].message.content.lower()[:1])\n",
    "    else:\n",
    "        for res in responses:\n",
    "            pred = res.choices[0].message.content.lower()\n",
    "            start_point = pred.find(\"prediction: \") + len(\"prediction: \")\n",
    "            if method == \"predict_then_explain\":\n",
    "                pred = pred[start_point: start_point + 1]\n",
    "            elif method == \"explain_then_predict\":\n",
    "                pred = pred[start_point: start_point + 1]\n",
    "            gpt_predictions.append(pred)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        truths = [json.loads(line)[\"output\"][:1] for line in f]\n",
    "    scores = [1 if gpt_predictions[j] == truths[j] else 0 for j in range(len(truths))]\n",
    "    return sum(scores) / len(truths)\n",
    "\n",
    "def z_test(n_sample, accuracy_1, accuracy_2):\n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    pooled = (accuracy_1 * n_sample + accuracy_2 * n_sample) / (n_sample * 2)\n",
    "    z_score = (accuracy_1 - accuracy_2) / np.sqrt(pooled * (1 - pooled) * 2 / n_sample)\n",
    "    p_value = norm.sf(abs(z_score)) * 2\n",
    "    return p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T03:22:13.440094Z",
     "start_time": "2024-10-01T03:22:13.429892Z"
    }
   },
   "id": "8daeb90383375fbc",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = \"finetune_llama3_1/op_test_alpaca.jsonl\"\n",
    "n_sample = len(op_responses_direct)\n",
    "\n",
    "op_accuracy_direct = prediction_accuracy(op_responses_direct, file_path)\n",
    "op_accuracy_pred_explain = prediction_accuracy(op_responses_pred_explain,\n",
    "                                               file_path,\n",
    "                                               method=\"predict_then_explain\")\n",
    "op_accuracy_explain_pred = prediction_accuracy(op_responses_explain_pred,\n",
    "                                               file_path,\n",
    "                                               method=\"explain_then_predict\")\n",
    "p_2 = z_test(n_sample, op_accuracy_direct, op_accuracy_pred_explain)\n",
    "p_3 = z_test(n_sample, op_accuracy_direct, op_accuracy_explain_pred)\n",
    "\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo is {op_accuracy_direct:.3f}.\")\n",
    "print(f\"Accuracy for predict-then-explain with GPT-3.5 turbo is {op_accuracy_pred_explain:.3f} with p-value: {p_2:.3f}\")\n",
    "print(f\"Accuracy for explain-then-predict with GPT-3.5 turbo is {op_accuracy_explain_pred:.3f} with p-value: {p_3:.3f}\")"
   ],
   "id": "98bee9c8e31c3bae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T03:22:51.582762Z",
     "start_time": "2024-10-01T03:22:51.547541Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for direct prediction with GPT-3.5 turbo is 0.485.\n",
      "Accuracy for direct prediction with GPT-3.5 turbo with knowledge is 0.530, with p-value: 0.368\n",
      "Accuracy for predict-then-explain with GPT-3.5 turbo is 0.470 with p-value: 0.764\n",
      "Accuracy for explain-then-predict with GPT-3.5 turbo is 0.455 with p-value: 0.548\n"
     ]
    }
   ],
   "execution_count": 100,
   "source": [
    "file_path = \"finetune_llama3_1/pairs_test_alpaca.jsonl\"\n",
    "n_sample = len(pairs_responses_direct)\n",
    "\n",
    "pairs_accuracy_direct = prediction_accuracy(pairs_responses_direct, file_path)\n",
    "pairs_accuracy_with_knowledge = prediction_accuracy(pairs_responses_with_knowledge, file_path)\n",
    "pairs_accuracy_pred_explain = prediction_accuracy(pairs_responses_pred_explain,\n",
    "                                               file_path,\n",
    "                                               method=\"predict_then_explain\")\n",
    "pairs_accuracy_explain_pred = prediction_accuracy(pairs_responses_explain_pred,\n",
    "                                               file_path,\n",
    "                                               method=\"explain_then_predict\")\n",
    "p_1 = z_test(n_sample, pairs_accuracy_direct, pairs_accuracy_with_knowledge)\n",
    "p_2 = z_test(n_sample, pairs_accuracy_direct, pairs_accuracy_pred_explain)\n",
    "p_3 = z_test(n_sample, pairs_accuracy_direct, pairs_accuracy_explain_pred)\n",
    "\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo is {pairs_accuracy_direct:.3f}.\")\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo with knowledge is {pairs_accuracy_with_knowledge:.3f}, with p-value: {p_1:.3f}\")\n",
    "print(f\"Accuracy for predict-then-explain with GPT-3.5 turbo is {pairs_accuracy_pred_explain:.3f} with p-value: {p_2:.3f}\")\n",
    "print(f\"Accuracy for explain-then-predict with GPT-3.5 turbo is {pairs_accuracy_explain_pred:.3f} with p-value: {p_3:.3f}\")"
   ],
   "id": "b37d46239074edb"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "def get_explanation_for_finetuning(prompts, truths, dataset=\"op\"):\n",
    "    responses = []\n",
    "    for prompt, truth in tqdm(zip(prompts, truths), \n",
    "                              desc=\"Getting GPT3.5 Responses: \", \n",
    "                              total=len(prompts)):\n",
    "        if dataset == \"op\":\n",
    "            if truth == \"malleable\":\n",
    "                insert = \"We know that he/she did get persuaded by some commentators. How might his/her speeching style and lexical features suggest he/she is malleable to persuasion?\"\n",
    "                \n",
    "            elif truth == \"resistant\":\n",
    "                insert = \"We know that he/she never get persuaded by others. How might his/her speeching style and lexical features suggest he/she is resistant to persuasion?\"\n",
    "                \n",
    "            instruction = f\"You're a semantic analyst. Now I will show you a person's opinion statement, who publicly announced his/her argument and encouraged other people to challenge it. {insert} Very briefly explain your analysis with no more than 2 paragraphs.\"\n",
    "        \n",
    "        elif dataset == \"pairs\":\n",
    "            instruction = f\"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. We know that the {truth} reply successfully persuaded the poster. How might his/her speeching style and lexical features suggest his/her persuasiveness? {knowledge} Very briefly explain your analysis with no more than 1000 tokens.\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=[\n",
    "                          {\"role\":\"system\", \"content\": instruction},\n",
    "                          prompt['body']['messages'][1]\n",
    "                      ],\n",
    "                    max_completion_tokens=prompt['body']['max_tokens']\n",
    "                    )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {prompt['custom_id']}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T03:24:44.427274Z",
     "start_time": "2024-10-01T03:24:44.419784Z"
    }
   },
   "id": "69b9ebf9fb986218"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "op_explanation_prompts = load_prompts(\"finetune_llama3_1/op_train_alpaca.jsonl\")\n",
    "op_truths = [line[\"output\"] for line in op_explanation_prompts]\n",
    "op_train_prompts = load_prompts(\"op_train_gpt.jsonl\")\n",
    "op_explanations = get_explanation_for_finetuning(op_train_prompts, op_truths, dataset=\"op\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bbf0b61f06aaa0e"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting GPT3.5 Responses: 100%|██████████| 1000/1000 [44:55<00:00,  2.70s/it] \n"
     ]
    }
   ],
   "source": [
    "pairs_explanation_prompts = load_prompts(\"finetune_llama3_1/pairs_train_alpaca.jsonl\")\n",
    "pairs_truths = [line[\"output\"] for line in pairs_explanation_prompts]\n",
    "pairs_train_prompts = load_prompts(\"pairs_train_gpt.jsonl\")\n",
    "pairs_explanations = get_explanation_for_finetuning(pairs_train_prompts, pairs_truths, dataset=\"pairs\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T04:14:04.873862Z",
     "start_time": "2024-10-01T03:29:09.191711Z"
    }
   },
   "id": "bf4ba53d57ec4ca0"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def create_jsonl(explanations, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for line in explanations:\n",
    "            content = line.choices[0].message.content\n",
    "            json.dump(content, f)\n",
    "            f.write(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T04:18:28.859531Z",
     "start_time": "2024-10-01T04:18:28.846481Z"
    }
   },
   "id": "fb7f443409af0693"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = 'finetune_llama3_1/op_gpt_explanations.jsonl'\n",
    "create_jsonl(op_explanations, file_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "525ad0384181a466"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "file_path = 'finetune_llama3_1/pairs_gpt_explanations.jsonl'\n",
    "create_jsonl(pairs_explanations, file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T04:18:30.504272Z",
     "start_time": "2024-10-01T04:18:30.479791Z"
    }
   },
   "id": "25950c320d1cb56e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ffb8b460bfd83265"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
