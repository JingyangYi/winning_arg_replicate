{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key='your-key-here')\n",
    "\n",
    "# Load prompts from a JSONL file\n",
    "def load_prompts(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "op_prompts = load_prompts('op_test_gpt.jsonl')\n",
    "pairs_prompts = load_prompts('pairs_test_gpt.jsonl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T17:32:26.622328Z",
     "start_time": "2024-10-01T17:32:26.604149Z"
    }
   },
   "id": "f02ac414ef772fff",
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to send requests and gather responses\n",
    "def get_responses(prompts, instruction, desc=\"Getting GPT3.5 Responses: \"):\n",
    "    responses = []\n",
    "    for prompt in tqdm(prompts, desc=desc):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=[\n",
    "                          {\"role\":\"system\", \"content\":instruction},\n",
    "                          prompt['body']['messages'][1]\n",
    "                      ],\n",
    "                    max_completion_tokens=prompt['body']['max_tokens'],\n",
    "                    )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {prompt['custom_id']}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T17:13:08.361440Z",
     "start_time": "2024-10-01T17:13:08.358485Z"
    }
   },
   "id": "69a03815dbdf6e2a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# Get responses of Original Posts from GPT3.5\n",
    "directly_predict = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? Answer with 'malleable' or 'resistant'.\"\n",
    "predict_then_explain = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? Answer with 'malleable' or 'resistant' and explain your answer. Response with the following format: Prediction: resistant/malleable \\n Explanation: briefly explain here.\"\n",
    "explain_then_predict = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? First briefly explain your analysis and then give your answer with resistant/malleable. Response with the following format: Explanation: briefly explain here. \\n  Prediction: resistant/malleable\"\n",
    "\n",
    "op_responses_direct = get_responses(op_prompts, directly_predict)\n",
    "op_responses_pred_explain = get_responses(op_prompts, predict_then_explain)\n",
    "op_responses_explain_pred = get_responses(op_prompts, explain_then_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:07:02.542363Z",
     "start_time": "2024-09-29T16:56:24.373934Z"
    }
   },
   "id": "b85ddc576407e332"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T17:13:12.916562Z",
     "start_time": "2024-10-01T17:13:12.914198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Knowledge on how to persuade the poster, extracted from the paper: Winning Arguments: Interaction Dynamics and Persuasion\n",
    "# Strategies in Good-faith Online Discussions\n",
    "knowledge = \"Hint: 1. Language Dissimilarity with Original Post: Persuasive replies use different content words but match in stopwords 2.Reply Length: Longer replies tend to be more persuasive, as they can convey more information and elaborate on points effectively. 3. Language Dissimilarity with Original Post: Persuasive replies use different content words but match in stopwords. 4. Links and Evidence: Including links as evidence in an argument increases the chances of persuasion. 5. Calmer Tone: Replies that use calmer, less intense language are more likely to persuade, as they come across as more composed. 6. Positive Emotion and Sentiment: Persuasive replies include a mix of positive and negative sentiment.\""
   ],
   "id": "cb9aa35543cfe6be",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T17:13:18.865272Z",
     "start_time": "2024-10-01T17:13:14.809118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get responses of Pairs(op, reply1, reply2) from GPT3.5\n",
    "directly_predict = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now judge which response succeeded in persuading. Answer only with first/second.\"\n",
    "predict_then_explain = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now first judge which response succeeded in persuading and then explain your analysis very briefly. Response with the following format: Prediction: answer only with first or second \\n Explanation: briefly explain here.\"\n",
    "explain_then_predict = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now first show your analysis very briefly and then judge which response succeeded in persuading. Response with the following format: Explanation: briefly analyse here. \\n Prediction: answer only with first or second\"\n",
    "\n",
    "pairs_responses_direct = get_responses(pairs_prompts, directly_predict)\n",
    "pairs_responses_pred_explain = get_responses(pairs_prompts, predict_then_explain)\n",
    "pairs_responses_explain_pred = get_responses(pairs_prompts, explain_then_predict)\n",
    "pairs_responses_with_knowledge = get_responses(pairs_prompts, directly_predict + knowledge)"
   ],
   "id": "10629898ab7633f4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting GPT3.5 Responses:   4%|▍         | 9/200 [00:03<01:24,  2.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m predict_then_explain \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt. Now first judge which response succeeded in persuading and then explain your analysis very briefly. Response with the following format: Prediction: answer only with first or second \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m Explanation: briefly explain here.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m explain_then_predict \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt. Now first show your analysis very briefly and then judge which response succeeded in persuading. Response with the following format: Explanation: briefly analyse here. \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m Prediction: answer only with first or second\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m pairs_responses_direct \u001B[38;5;241m=\u001B[39m get_responses(pairs_prompts, directly_predict)\n\u001B[1;32m      7\u001B[0m pairs_responses_pred_explain \u001B[38;5;241m=\u001B[39m get_responses(pairs_prompts, predict_then_explain)\n\u001B[1;32m      8\u001B[0m pairs_responses_explain_pred \u001B[38;5;241m=\u001B[39m get_responses(pairs_prompts, explain_then_predict)\n",
      "Cell \u001B[0;32mIn[28], line 6\u001B[0m, in \u001B[0;36mget_responses\u001B[0;34m(prompts, instruction, desc)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m tqdm(prompts, desc\u001B[38;5;241m=\u001B[39mdesc):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 6\u001B[0m         response \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mchat\u001B[38;5;241m.\u001B[39mcompletions\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m      7\u001B[0m                   model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-3.5-turbo\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m                   messages\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m      9\u001B[0m                       {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m:instruction},\n\u001B[1;32m     10\u001B[0m                       prompt[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     11\u001B[0m                   ],\n\u001B[1;32m     12\u001B[0m                 max_completion_tokens\u001B[38;5;241m=\u001B[39mprompt[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     13\u001B[0m                 )\n\u001B[1;32m     14\u001B[0m         responses\u001B[38;5;241m.\u001B[39mappend(response)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    272\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py:704\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    668\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    701\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    702\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m    703\u001B[0m     validate_response_format(response_format)\n\u001B[0;32m--> 704\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m    705\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    706\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[1;32m    707\u001B[0m             {\n\u001B[1;32m    708\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[1;32m    709\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[1;32m    710\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[1;32m    711\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n\u001B[1;32m    712\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n\u001B[1;32m    713\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n\u001B[1;32m    714\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n\u001B[1;32m    715\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_completion_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_completion_tokens,\n\u001B[1;32m    716\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[1;32m    717\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n\u001B[1;32m    718\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparallel_tool_calls\u001B[39m\u001B[38;5;124m\"\u001B[39m: parallel_tool_calls,\n\u001B[1;32m    719\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n\u001B[1;32m    720\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[1;32m    721\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[1;32m    722\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mservice_tier\u001B[39m\u001B[38;5;124m\"\u001B[39m: service_tier,\n\u001B[1;32m    723\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[1;32m    724\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[1;32m    725\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream_options,\n\u001B[1;32m    726\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[1;32m    727\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[1;32m    728\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[1;32m    729\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n\u001B[1;32m    730\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[1;32m    731\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n\u001B[1;32m    732\u001B[0m             },\n\u001B[1;32m    733\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParams,\n\u001B[1;32m    734\u001B[0m         ),\n\u001B[1;32m    735\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m    736\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    737\u001B[0m         ),\n\u001B[1;32m    738\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n\u001B[1;32m    739\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    740\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n\u001B[1;32m    741\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1270\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1257\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1258\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1265\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1266\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1267\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1268\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1269\u001B[0m     )\n\u001B[0;32m-> 1270\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:947\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    944\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    945\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 947\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m    948\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m    949\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m    950\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    951\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    952\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m    953\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:983\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m    980\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSending HTTP Request: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, request\u001B[38;5;241m.\u001B[39mmethod, request\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 983\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39msend(\n\u001B[1;32m    984\u001B[0m         request,\n\u001B[1;32m    985\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_stream_response_body(request\u001B[38;5;241m=\u001B[39mrequest),\n\u001B[1;32m    986\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    987\u001B[0m     )\n\u001B[1;32m    988\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    989\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered httpx.TimeoutException\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpx/_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[0;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[1;32m    906\u001B[0m follow_redirects \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    907\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfollow_redirects\n\u001B[1;32m    908\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(follow_redirects, UseClientDefault)\n\u001B[1;32m    909\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m follow_redirects\n\u001B[1;32m    910\u001B[0m )\n\u001B[1;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[0;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_handling_auth(\n\u001B[1;32m    915\u001B[0m     request,\n\u001B[1;32m    916\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[1;32m    917\u001B[0m     follow_redirects\u001B[38;5;241m=\u001B[39mfollow_redirects,\n\u001B[1;32m    918\u001B[0m     history\u001B[38;5;241m=\u001B[39m[],\n\u001B[1;32m    919\u001B[0m )\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    921\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpx/_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[0;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[1;32m    939\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[1;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_handling_redirects(\n\u001B[1;32m    943\u001B[0m         request,\n\u001B[1;32m    944\u001B[0m         follow_redirects\u001B[38;5;241m=\u001B[39mfollow_redirects,\n\u001B[1;32m    945\u001B[0m         history\u001B[38;5;241m=\u001B[39mhistory,\n\u001B[1;32m    946\u001B[0m     )\n\u001B[1;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    948\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpx/_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[0;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[1;32m    976\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    977\u001B[0m     hook(request)\n\u001B[0;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_single_request(request)\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    981\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpx/_client.py:1015\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m   1010\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1011\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1012\u001B[0m     )\n\u001B[1;32m   1014\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[0;32m-> 1015\u001B[0m     response \u001B[38;5;241m=\u001B[39m transport\u001B[38;5;241m.\u001B[39mhandle_request(request)\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n\u001B[1;32m   1019\u001B[0m response\u001B[38;5;241m.\u001B[39mrequest \u001B[38;5;241m=\u001B[39m request\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpx/_transports/default.py:233\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    220\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[1;32m    221\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    222\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    230\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    231\u001B[0m )\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m--> 233\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool\u001B[38;5;241m.\u001B[39mhandle_request(req)\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[1;32m    238\u001B[0m     status_code\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mstatus,\n\u001B[1;32m    239\u001B[0m     headers\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m    240\u001B[0m     stream\u001B[38;5;241m=\u001B[39mResponseStream(resp\u001B[38;5;241m.\u001B[39mstream),\n\u001B[1;32m    241\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    242\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:268\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ShieldCancellation():\n\u001B[1;32m    267\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse_closed(status)\n\u001B[0;32m--> 268\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:251\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    248\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 251\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39mhandle_request(request)\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001B[39;00m\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;66;03m# indicates we need to retry the request on a new connection.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    258\u001B[0m     \u001B[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001B[39;00m\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# up as HTTP/1.1.\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool_lock:\n\u001B[1;32m    261\u001B[0m         \u001B[38;5;66;03m# Maintain our position in the request queue, but reset the\u001B[39;00m\n\u001B[1;32m    262\u001B[0m         \u001B[38;5;66;03m# status so that the request becomes queued again.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ConnectionNotAvailable()\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection\u001B[38;5;241m.\u001B[39mhandle_request(request)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py:133\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_closed\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed()\n\u001B[0;32m--> 133\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py:111\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs\n\u001B[1;32m    105\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    106\u001B[0m     (\n\u001B[1;32m    107\u001B[0m         http_version,\n\u001B[1;32m    108\u001B[0m         status,\n\u001B[1;32m    109\u001B[0m         reason_phrase,\n\u001B[1;32m    110\u001B[0m         headers,\n\u001B[0;32m--> 111\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_receive_response_headers(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    112\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    113\u001B[0m         http_version,\n\u001B[1;32m    114\u001B[0m         status,\n\u001B[1;32m    115\u001B[0m         reason_phrase,\n\u001B[1;32m    116\u001B[0m         headers,\n\u001B[1;32m    117\u001B[0m     )\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[1;32m    120\u001B[0m     status\u001B[38;5;241m=\u001B[39mstatus,\n\u001B[1;32m    121\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    127\u001B[0m     },\n\u001B[1;32m    128\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py:176\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_headers\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    173\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_receive_event(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mResponse):\n\u001B[1;32m    178\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py:212\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    209\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[0;32m--> 212\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_stream\u001B[38;5;241m.\u001B[39mread(\n\u001B[1;32m    213\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mREAD_NUM_BYTES, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    214\u001B[0m     )\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[0;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv(max_bytes)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/ssl.py:1233\u001B[0m, in \u001B[0;36mSSLSocket.recv\u001B[0;34m(self, buflen, flags)\u001B[0m\n\u001B[1;32m   1229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1230\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1231\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1232\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread(buflen)\n\u001B[1;32m   1234\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv(buflen, flags)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/ssl.py:1106\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[1;32m   1105\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1106\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[1;32m   1108\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_ragged_eofs:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T17:50:24.603305Z",
     "start_time": "2024-10-01T17:33:04.103736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instruction_1 = f\"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. Analyze the persuasiveness of the two replies respectively in two paragraphs with the following format: First Reply: your analysis here.\\n Second Reply: your analysis here.\\n Note only analysis, not conclusions. \\n {knowledge}\"\n",
    "instruction_2 = f\"You're a semantic analyst. The following two paragraphs are analysis on the persuasiveness of two replies. These replies are from an online discussion community that are trying to convince the original poster to revise its opinion. Based on these analysis, which reply do you think that successfully persuaded the original poster? \\n Answer only with first or second.\\n {knowledge}\"\n",
    "# Implement a two stage prompting: \n",
    "# 1) ask GPT to analyse the linguistic features of the two replies but do not make conclusions. \n",
    "# 2) ask GPT to evaluate previous analysis and make conclusions. \n",
    "# Both stages have knowledge instilled.\n",
    "def get_two_stage_responses(prompts, instruction_1, instruction_2):\n",
    "    stage_one_responses = get_responses(prompts, instruction_1, desc=\"Getting GPT3.5 Responses(1st stage): \")\n",
    "    stage_one_responses = [res.choices[0].message.content for res in stage_one_responses]\n",
    "    stage_two_responses = []\n",
    "    for prompt in tqdm(stage_one_responses, desc=\"Getting GPT3.5 Responses(2nd stage): \"):\n",
    "        response = client.chat.completions.create(\n",
    "                  model=\"gpt-3.5-turbo\",\n",
    "                  messages=[\n",
    "                      {\"role\":\"system\", \"content\":instruction_2},\n",
    "                      {\"role\": \"user\", \"content\":prompt},\n",
    "                  ],\n",
    "                max_completion_tokens=1000,\n",
    "                )\n",
    "        stage_two_responses.append(response)\n",
    "    return stage_two_responses\n",
    "\n",
    "paris_two_stage_responses = get_two_stage_responses(pairs_prompts, instruction_1, instruction_2)"
   ],
   "id": "a488f109a070fbfc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting GPT3.5 Responses(1st stage): 100%|██████████| 200/200 [15:58<00:00,  4.79s/it]\n",
      "Getting GPT3.5 Responses(2nd stage): 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "def prediction_accuracy(responses, file_path, test_n_sample=200, method=\"direct\"):\n",
    "    gpt_predictions = []\n",
    "    if method == \"direct\":\n",
    "        for res in responses:\n",
    "            gpt_predictions.append(res.choices[0].message.content.lower()[:1])\n",
    "    else:\n",
    "        for res in responses:\n",
    "            pred = res.choices[0].message.content.lower()\n",
    "            start_point = pred.find(\"prediction: \") + len(\"prediction: \")\n",
    "            if method == \"predict_then_explain\":\n",
    "                pred = pred[start_point: start_point + 1]\n",
    "            elif method == \"explain_then_predict\":\n",
    "                pred = pred[start_point: start_point + 1]\n",
    "            gpt_predictions.append(pred)\n",
    "    \n",
    "    def check_pred(preds):\n",
    "        n_mismatches = 0\n",
    "        for i, p in (enumerate(preds)):\n",
    "            if p not in ['f', 's']:\n",
    "                if p not in ['m', 'r']:\n",
    "                    n_mismatches += 1\n",
    "        print(f\"{n_mismatches}/{len(preds)} of the predictions are not in correct format! They will not be included in accuracy.\")\n",
    "        return n_mismatches\n",
    "    n_mismatch = check_pred(gpt_predictions)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        truths = [json.loads(line)[\"output\"][:1] for line in f][:test_n_sample]\n",
    "    scores = [1 if gpt_predictions[j] == truths[j] else 0 for j in range(len(truths))]\n",
    "    \n",
    "    return sum(scores) - n_mismatch / len(truths) - n_mismatch\n",
    "\n",
    "def z_test(n_sample, accuracy_1, accuracy_2):\n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    pooled = (accuracy_1 * n_sample + accuracy_2 * n_sample) / (n_sample * 2)\n",
    "    z_score = (accuracy_1 - accuracy_2) / np.sqrt(pooled * (1 - pooled) * 2 / n_sample)\n",
    "    p_value = norm.sf(abs(z_score)) * 2\n",
    "    return p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T17:50:27.972371Z",
     "start_time": "2024-10-01T17:50:27.969299Z"
    }
   },
   "id": "8daeb90383375fbc",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = \"finetune_llama3_1/op_test_alpaca.jsonl\"\n",
    "n_sample = len(op_responses_direct)\n",
    "\n",
    "op_accuracy_direct = prediction_accuracy(op_responses_direct, file_path)\n",
    "op_accuracy_pred_explain = prediction_accuracy(op_responses_pred_explain,\n",
    "                                               file_path,\n",
    "                                               method=\"predict_then_explain\")\n",
    "op_accuracy_explain_pred = prediction_accuracy(op_responses_explain_pred,\n",
    "                                               file_path,\n",
    "                                               method=\"explain_then_predict\")\n",
    "p_2 = z_test(n_sample, op_accuracy_direct, op_accuracy_pred_explain)\n",
    "p_3 = z_test(n_sample, op_accuracy_direct, op_accuracy_explain_pred)\n",
    "\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo is {op_accuracy_direct:.3f}.\")\n",
    "print(f\"Accuracy for predict-then-explain with GPT-3.5 turbo is {op_accuracy_pred_explain:.3f} with p-value: {p_2:.3f}\")\n",
    "print(f\"Accuracy for explain-then-predict with GPT-3.5 turbo is {op_accuracy_explain_pred:.3f} with p-value: {p_3:.3f}\")"
   ],
   "id": "98bee9c8e31c3bae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T03:22:51.582762Z",
     "start_time": "2024-10-01T03:22:51.547541Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for direct prediction with GPT-3.5 turbo is 0.485.\n",
      "Accuracy for direct prediction with GPT-3.5 turbo with knowledge is 0.530, with p-value: 0.368\n",
      "Accuracy for predict-then-explain with GPT-3.5 turbo is 0.470 with p-value: 0.764\n",
      "Accuracy for explain-then-predict with GPT-3.5 turbo is 0.455 with p-value: 0.548\n"
     ]
    }
   ],
   "execution_count": 100,
   "source": [
    "file_path = \"finetune_llama3_1/pairs_test_alpaca.jsonl\"\n",
    "n_sample = len(pairs_responses_direct)\n",
    "\n",
    "pairs_accuracy_direct = prediction_accuracy(pairs_responses_direct, file_path)\n",
    "pairs_accuracy_with_knowledge = prediction_accuracy(pairs_responses_with_knowledge, file_path)\n",
    "pairs_accuracy_pred_explain = prediction_accuracy(pairs_responses_pred_explain,\n",
    "                                               file_path,\n",
    "                                               method=\"predict_then_explain\")\n",
    "pairs_accuracy_explain_pred = prediction_accuracy(pairs_responses_explain_pred,\n",
    "                                               file_path,\n",
    "                                               method=\"explain_then_predict\")\n",
    "p_1 = z_test(n_sample, pairs_accuracy_direct, pairs_accuracy_with_knowledge)\n",
    "p_2 = z_test(n_sample, pairs_accuracy_direct, pairs_accuracy_pred_explain)\n",
    "p_3 = z_test(n_sample, pairs_accuracy_direct, pairs_accuracy_explain_pred)\n",
    "\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo is {pairs_accuracy_direct:.3f}.\")\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo with knowledge is {pairs_accuracy_with_knowledge:.3f}, with p-value: {p_1:.3f}\")\n",
    "print(f\"Accuracy for predict-then-explain with GPT-3.5 turbo is {pairs_accuracy_pred_explain:.3f} with p-value: {p_2:.3f}\")\n",
    "print(f\"Accuracy for explain-then-predict with GPT-3.5 turbo is {pairs_accuracy_explain_pred:.3f} with p-value: {p_3:.3f}\")"
   ],
   "id": "b37d46239074edb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T17:50:32.492225Z",
     "start_time": "2024-10-01T17:50:32.488572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"finetune_llama3_1/pairs_test_alpaca.jsonl\"\n",
    "n_sample = len(paris_two_stage_responses)\n",
    "pairs_accuracy_two_stage = prediction_accuracy(paris_two_stage_responses, file_path, test_n_sample=len(pairs_prompts))\n",
    "print(f\"Accuracy for two stage prediction with GPT-3.5 turbo is {pairs_accuracy_two_stage:.3f}.\")"
   ],
   "id": "12fa7af882ae0fb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 114 response is not in correct format!\n",
      "The 140 response is not in correct format!\n",
      "Accuracy for two stage prediction with GPT-3.5 turbo is 0.575.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "def get_explanation_for_finetuning(prompts, truths, dataset=\"op\"):\n",
    "    responses = []\n",
    "    for prompt, truth in tqdm(zip(prompts, truths), \n",
    "                              desc=\"Getting GPT3.5 Responses: \", \n",
    "                              total=len(prompts)):\n",
    "        if dataset == \"op\":\n",
    "            if truth == \"malleable\":\n",
    "                insert = \"We know that he/she did get persuaded by some commentators. How might his/her speeching style and lexical features suggest he/she is malleable to persuasion?\"\n",
    "                \n",
    "            elif truth == \"resistant\":\n",
    "                insert = \"We know that he/she never get persuaded by others. How might his/her speeching style and lexical features suggest he/she is resistant to persuasion?\"\n",
    "                \n",
    "            instruction = f\"You're a semantic analyst. Now I will show you a person's opinion statement, who publicly announced his/her argument and encouraged other people to challenge it. {insert} Very briefly explain your analysis with no more than 2 paragraphs.\"\n",
    "        \n",
    "        elif dataset == \"pairs\":\n",
    "            instruction = f\"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. We know that the {truth} reply successfully persuaded the poster. How might his/her speeching style and lexical features suggest his/her persuasiveness? {knowledge} Very briefly explain your analysis with no more than 1000 tokens.\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=[\n",
    "                          {\"role\":\"system\", \"content\": instruction},\n",
    "                          prompt['body']['messages'][1]\n",
    "                      ],\n",
    "                    max_completion_tokens=prompt['body']['max_tokens']\n",
    "                    )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {prompt['custom_id']}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T03:24:44.427274Z",
     "start_time": "2024-10-01T03:24:44.419784Z"
    }
   },
   "id": "69b9ebf9fb986218"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "op_explanation_prompts = load_prompts(\"finetune_llama3_1/op_train_alpaca.jsonl\")\n",
    "op_truths = [line[\"output\"] for line in op_explanation_prompts]\n",
    "op_train_prompts = load_prompts(\"op_train_gpt.jsonl\")\n",
    "op_explanations = get_explanation_for_finetuning(op_train_prompts, op_truths, dataset=\"op\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bbf0b61f06aaa0e"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting GPT3.5 Responses: 100%|██████████| 1000/1000 [44:55<00:00,  2.70s/it] \n"
     ]
    }
   ],
   "source": [
    "pairs_explanation_prompts = load_prompts(\"finetune_llama3_1/pairs_train_alpaca.jsonl\")\n",
    "pairs_truths = [line[\"output\"] for line in pairs_explanation_prompts]\n",
    "pairs_train_prompts = load_prompts(\"pairs_train_gpt.jsonl\")\n",
    "pairs_explanations = get_explanation_for_finetuning(pairs_train_prompts, pairs_truths, dataset=\"pairs\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T04:14:04.873862Z",
     "start_time": "2024-10-01T03:29:09.191711Z"
    }
   },
   "id": "bf4ba53d57ec4ca0"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def create_jsonl(explanations, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for line in explanations:\n",
    "            content = line.choices[0].message.content\n",
    "            json.dump(content, f)\n",
    "            f.write(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T04:18:28.859531Z",
     "start_time": "2024-10-01T04:18:28.846481Z"
    }
   },
   "id": "fb7f443409af0693"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = 'finetune_llama3_1/op_gpt_explanations.jsonl'\n",
    "create_jsonl(op_explanations, file_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "525ad0384181a466"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "file_path = 'finetune_llama3_1/pairs_gpt_explanations.jsonl'\n",
    "create_jsonl(pairs_explanations, file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T04:18:30.504272Z",
     "start_time": "2024-10-01T04:18:30.479791Z"
    }
   },
   "id": "25950c320d1cb56e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ffb8b460bfd83265"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
