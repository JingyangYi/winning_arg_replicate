{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T20:11:22.781167Z",
     "start_time": "2024-09-29T20:11:12.925846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ],
   "id": "e30a85313aca9fe0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Model**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81fce13b65a3bd5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:17:37.956357Z",
     "start_time": "2024-10-02T17:17:21.794370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 3000\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model_original, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ],
   "id": "d523c086728edf56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti SUPER. Max memory: 15.688 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# comments in this cell came from Unsloth tutorial\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model_original,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-02T17:17:37.957119Z"
    }
   },
   "id": "67d95eda7371e81f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Datasets for OPs and Pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f6b829b292a50fc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-02T17:17:39.739548Z"
    }
   },
   "id": "cd8cd9e0accb1147"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "op_instruction = \"You're a semantic analyst. Now I will show you a person's opinion statement, who publicly announced his/her argument and encouraged other people to challenge it. Judging from his/her speeching style and lexical features, do you think he/she is resistant or malleable to persuasion? First answer with resistant or malleable, then very briefly explain your analysis.\"\n",
    "\n",
    "pairs_instruction = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now judge which response succeeded in persuading.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-02T17:17:39.741889Z"
    }
   },
   "id": "930dccf9775dec18"
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"finetune_datasets/op_gpt_explanations.jsonl\", \"r\") as f:\n",
    "    import json\n",
    "    explanations = [json.loads(line) for line in f]\n",
    "\n",
    "# Knowledge are added to all the training in this notebook \n",
    "knowledge_op = \"Hints: 1. Use of First-Person Pronouns: The use of first-person singular pronouns (e.g., I, me) is strongly correlated with malleability while the use of first-person plural pronouns (e.g., we, us) is more associated with resistant opinions. 2. Dominance in Language: Higher dominance in the language used by the OP correlates with malleability. 3. Calm Tone: calmer, less emotional language in the original post is associated with malleability. 4. Valence (Emotional Positivity): Higher valence, which reflects more positive emotional tone, indicates malleability. 5.Formatting: Posts that are well-organized, with more paragraphs and formatting such as bolds and bullet lists, are correlated with malleable opinions.\"\n",
    "\n",
    "def formatting_prompts_func(dataset):\n",
    "    inputs = dataset[\"input\"]\n",
    "    outputs = [line + '\\n' + exp for line, exp in zip(dataset[\"output\"], explanations)]\n",
    "    instructions = [op_instruction + knowledge_op]  * len(inputs)\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "op_dataset = load_dataset(\"json\", data_files={\"train\": \"op_train_alpaca.jsonl\",\n",
    "                                           \"test\": \"op_test_alpaca.jsonl\",})\n",
    "\n",
    "op_dataset_train = op_dataset[\"train\"]\n",
    "op_dataset_train = op_dataset_train.map(formatting_prompts_func, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:17:52.020646Z",
     "start_time": "2024-10-02T17:17:51.799472Z"
    }
   },
   "id": "d1084372349b1a54",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"finetune_datasets/pairs_gpt_explanations.jsonl\", \"r\") as f:\n",
    "    import json\n",
    "    explanations = [json.loads(line) for line in f]\n",
    "\n",
    "# Knowledge are added to all the training in this notebook \n",
    "knowledge_pairs = \"Hint: 1. Reply Length: Longer replies tend to be more persuasive, as they can convey more information and elaborate on points effectively. 2. Language Dissimilarity with Original Post: Persuasive replies use different content words but match in stopwords. 3. Links and Evidence: Including links as evidence in an argument increases the chances of persuasion. 4. Calmer Tone: Replies that use calmer, less intense language are more likely to persuade, as they come across as more composed. 5. Positive Emotion and Sentiment: Persuasive replies include a mix of positive and negative sentiment.\"\n",
    "\n",
    "def formatting_prompts_func(dataset):\n",
    "    inputs = dataset[\"input\"]\n",
    "    outputs = [line + '\\n' + exp for line, exp in zip(dataset[\"output\"], explanations)]\n",
    "    instructions = [pairs_instruction + knowledge_pairs]  * len(inputs)\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "pairs_dataset = load_dataset(\"json\", data_files={\"train\": \"pairs_train_alpaca.jsonl\",\n",
    "                                           \"test\": \"pairs_test_alpaca.jsonl\",})\n",
    "\n",
    "pairs_dataset_train = pairs_dataset[\"train\"]\n",
    "pairs_dataset_train = pairs_dataset_train.map(formatting_prompts_func, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:18:13.620098Z",
     "start_time": "2024-10-02T17:18:13.414126Z"
    }
   },
   "id": "a9e0beb16d88d3ec",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38e06508546ce647"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:18:30.194353Z",
     "start_time": "2024-10-02T17:18:28.715189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = op_dataset_train, \n",
    "    # train_dataset = pairs_dataset_train, # Uncomment this to finetune with pairs data\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ],
   "id": "e95e112af4177681",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cb8c1e7a8174ed8a66653f0f0b308e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:24:15.453652Z",
     "start_time": "2024-10-02T17:18:30.195087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer_stats = trainer.train()"
   ],
   "id": "3f8a93b9d366009f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 05:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.397500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.396600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.430500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.428500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.105200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:24:15.456316Z",
     "start_time": "2024-10-02T17:24:15.454214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "def infer(dataset, model, instruction):\n",
    "  results = []\n",
    "  for line in tqdm(dataset, desc=\"Inferring on test set: \"):\n",
    "      inputs = tokenizer(\n",
    "      [\n",
    "          alpaca_prompt.format(\n",
    "              instruction,\n",
    "              line[\"input\"], \n",
    "              \"\") \n",
    "      ], return_tensors = \"pt\").to(\"cuda\")\n",
    "      outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "      result = tokenizer.batch_decode(outputs)[0]\n",
    "      start_mark = \"### Response:\\n\"\n",
    "      start_point = result.find(start_mark) + len(start_mark)\n",
    "      result = result[start_point: start_point + 1].lower()\n",
    "\n",
    "      results.append(result)\n",
    "  return results"
   ],
   "id": "b9d17543c8f054c9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inferences on Test Set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d637707462c1501d"
  },
  {
   "cell_type": "code",
   "source": [
    "# OP model inferences\n",
    "op_test_dataset = op_dataset[\"test\"]\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "op_ft_results = infer(op_test_dataset, model, op_instruction + knowledge_op)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:28:22.348951Z",
     "start_time": "2024-10-02T17:24:15.456703Z"
    }
   },
   "id": "a08d3ef577ce4ba1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring on test set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:06<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# Pairs model inferences\n",
    "pairs_test_dataset = pairs_dataset[\"test\"]\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "pairs_ft_results = infer(pairs_test_dataset, model, pairs_instruction + knowledge_pairs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:11:47.934797Z",
     "start_time": "2024-10-02T17:07:00.404237Z"
    }
   },
   "id": "7dfe21c0f0627357",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring on test set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:47<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def accuracy(results, targets):\n",
    "    results = [result.lower()[:1] for result in results]\n",
    "    \n",
    "    # check if all answers are generated in correct format\n",
    "    def check_pred(preds):\n",
    "        n_mismatches = 0\n",
    "        for i, p in (enumerate(preds)):\n",
    "            if p not in ['f', 's']:\n",
    "                if p not in ['m', 'r']:\n",
    "                    n_mismatches += 1\n",
    "        if n_mismatches:\n",
    "            print(f\"{n_mismatches}/{len(preds)} \"\n",
    "              f\"of the predictions are not in correct format! \"\n",
    "              f\"They will not be included in counting accuracy.\")\n",
    "        return n_mismatches\n",
    "    n_mismatches = check_pred(results)\n",
    "    \n",
    "    correct_count = 0\n",
    "    total = len(results)\n",
    "    for i in range(total):\n",
    "      if results[i] == targets[i]:\n",
    "        correct_count += 1\n",
    "\n",
    "    return (correct_count - n_mismatches) / (total - n_mismatches)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:28:22.351857Z",
     "start_time": "2024-10-02T17:28:22.349559Z"
    }
   },
   "id": "6b703ac2ef9b3452",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:28:22.367560Z",
     "start_time": "2024-10-02T17:28:22.352285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Accuracy of OP model\n",
    "truths= [line[\"output\"][:1] for line in op_test_dataset]\n",
    "\n",
    "accu_ft = accuracy(op_ft_results, truths)\n",
    "print(f\"The finetuned model accuracy on test set is {accu_ft:.3f}\")"
   ],
   "id": "ae48164547c76ce9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The finetuned model accuracy on test set is 0.565\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:11:47.953424Z",
     "start_time": "2024-10-02T17:11:47.938067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Accuracy of Pairs model\n",
    "truths= [line[\"output\"][:1] for line in pairs_test_dataset]\n",
    "\n",
    "accu_ft = accuracy(pairs_ft_results, truths)\n",
    "print(f\"The finetuned model accuracy on test set is {accu_ft:.3f}\")"
   ],
   "id": "78111501ef00e121",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The finetuned model accuracy on test set is 0.530\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "82ba54ef7951a277"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
