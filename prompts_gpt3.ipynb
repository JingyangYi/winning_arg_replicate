{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key='your api key')\n",
    "\n",
    "# Load prompts from a JSONL file\n",
    "def load_prompts(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "op_prompts = load_prompts('op_test_gpt.jsonl')\n",
    "pairs_prompts = load_prompts('pairs_test_gpt.jsonl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T17:40:05.261545Z",
     "start_time": "2024-09-30T17:40:05.039311Z"
    }
   },
   "id": "f02ac414ef772fff",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to send requests and gather responses\n",
    "def get_responses(prompts, instruction):\n",
    "    responses = []\n",
    "    for prompt in tqdm(prompts, desc=\"Getting GPT3.5 Responses: \"):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=[\n",
    "                          {\"role\":\"system\", \"content\":instruction},\n",
    "                          prompt['body']['messages'][1]\n",
    "                      ],\n",
    "                    max_completion_tokens=prompt['body']['max_tokens']\n",
    "                    )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {prompt['custom_id']}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T17:40:07.620455Z",
     "start_time": "2024-09-30T17:40:07.618313Z"
    }
   },
   "id": "69a03815dbdf6e2a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# Get responses of Original Posts from GPT3.5\n",
    "directly_predict = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? Answer with 'malleable' or 'resistant'.\"\n",
    "predict_then_explain = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? Answer with 'malleable' or 'resistant' and explain your answer. Response with the following format: Prediction: resistant/malleable \\n Explanation: briefly explain here.\"\n",
    "explain_then_predict = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? First briefly explain your analysis and then give your answer with resistant/malleable. Response with the following format: Explanation: briefly explain here. \\n  Prediction: resistant/malleable\"\n",
    "\n",
    "op_responses_direct = get_responses(op_prompts, directly_predict)\n",
    "op_responses_pred_explain = get_responses(op_prompts, predict_then_explain)\n",
    "op_responses_explain_pred = get_responses(op_prompts, explain_then_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:07:02.542363Z",
     "start_time": "2024-09-29T16:56:24.373934Z"
    }
   },
   "id": "b85ddc576407e332"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:55:55.361212Z",
     "start_time": "2024-09-30T17:40:25.409594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get responses of Pairs(op, reply1, reply2) from GPT3.5\n",
    "directly_predict = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now judge which response succeeded in persuading. Answer reply 1 or reply 1 only.\"\n",
    "predict_then_explain = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now first judge which response succeeded in persuading and then explain your analysis. Response with the following format: Prediction: reply 1/reply 2 \\n Explanation: briefly explain here.\"\n",
    "explain_then_predict = \"This is a conversation from an online discussion community. The first was a poster who posted an opinion, and the next two replies were each trying to convince the poster to revise his opinion. The two responses were similar, but one managed to convince the poster and the other didn't. Now first show your analysis and then judge which response succeeded in persuading. Response with the following format: Explanation: briefly explain here. \\n Prediction: reply 1/reply 2\"\n",
    "\n",
    "pairs_responses_direct = get_responses(pairs_prompts, directly_predict)\n",
    "pairs_responses_pred_explain = get_responses(pairs_prompts, predict_then_explain)\n",
    "pairs_responses_explain_pred = get_responses(pairs_prompts, explain_then_predict)"
   ],
   "id": "10629898ab7633f4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting GPT3.5 Responses: 100%|██████████| 200/200 [01:38<00:00,  2.03it/s]\n",
      "Getting GPT3.5 Responses: 100%|██████████| 200/200 [06:20<00:00,  1.90s/it]\n",
      "Getting GPT3.5 Responses: 100%|██████████| 200/200 [07:30<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "def prediction_accuracy(responses, file_path, method=\"direct\"):\n",
    "    gpt_predictions = []\n",
    "    if method == \"direct\":\n",
    "        for res in responses:\n",
    "            gpt_predictions.append(res.choices[0].message.content.lower())\n",
    "    else:\n",
    "        for res in responses:\n",
    "            pred = res.choices[0].message.content.lower()\n",
    "            start_point = pred.find(\"prediction: \") + len(\"prediction: \")\n",
    "            # the strings 'malleable' and 'resistant' happen to have the same length of 9\n",
    "            if method == \"predict_then_explain\":\n",
    "                pred = pred[start_point: start_point + 9]\n",
    "            elif method == \"explain_then_predict\":\n",
    "                pred = pred[start_point: start_point + 9]\n",
    "            gpt_predictions.append(pred)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        truths = [json.loads(line)[\"output\"] for line in f]\n",
    "    scores = [1 if gpt_predictions[j] == truths[j] else 0 for j in range(len(truths))]\n",
    "    return sum(scores) / len(truths)\n",
    "\n",
    "def z_test(n_sample, accuracy_1, accuracy_2):\n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    pooled = (accuracy_1 * n_sample + accuracy_2 * n_sample) / (n_sample * 2)\n",
    "    z_score = (accuracy_1 - accuracy_2) / np.sqrt(pooled * (1 - pooled) * 2 / n_sample)\n",
    "    p_value = norm.sf(abs(z_score)) * 2\n",
    "    return p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T17:57:17.476175Z",
     "start_time": "2024-09-30T17:57:17.472310Z"
    }
   },
   "id": "8daeb90383375fbc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = \"finetune_llama3_1/pairs_test_alpaca.jsonl\"\n",
    "n_sample = len(op_responses_direct)\n",
    "\n",
    "op_accuracy_direct = prediction_accuracy(op_responses_direct, file_path)\n",
    "op_accuracy_pred_explain = prediction_accuracy(op_responses_pred_explain,\n",
    "                                               file_path,\n",
    "                                               method=\"predict_then_explain\")\n",
    "op_accuracy_explain_pred = prediction_accuracy(op_responses_explain_pred,\n",
    "                                               file_path,\n",
    "                                               method=\"explain_then_predict\")\n",
    "p_2 = z_test(n_sample, op_accuracy_direct, op_accuracy_pred_explain)\n",
    "p_3 = z_test(n_sample, op_accuracy_direct, op_accuracy_explain_pred)\n",
    "\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo is {op_accuracy_direct:.3f}.\")\n",
    "print(f\"Accuracy for predict-then-explain with GPT-3.5 turbo is {op_accuracy_pred_explain:.3f} with p-value: {p_2:.3f}\")\n",
    "print(f\"Accuracy for explain-then-predict with GPT-3.5 turbo is {op_accuracy_explain_pred:.3f} with p-value: {p_3:.3f}\")"
   ],
   "id": "98bee9c8e31c3bae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pairs_accuracy_direct = prediction_accuracy(pairs_responses_direct, file_path)\n",
    "pairs_accuracy_pred_explain = prediction_accuracy(pairs_responses_pred_explain,\n",
    "                                               file_path,\n",
    "                                               method=\"predict_then_explain\")\n",
    "pairs_accuracy_explain_pred = prediction_accuracy(pairs_responses_explain_pred,\n",
    "                                               file_path,\n",
    "                                               method=\"explain_then_predict\")\n",
    "p_2 = z_test(n_sample, pairs_accuracy_direct, op_accuracy_pred_explain)\n",
    "p_3 = z_test(n_sample, pairs_accuracy_direct, op_accuracy_explain_pred)\n",
    "\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo is {pairs_accuracy_direct:.3f}.\")\n",
    "print(f\"Accuracy for predict-then-explain with GPT-3.5 turbo is {pairs_accuracy_pred_explain:.3f} with p-value: {p_2:.3f}\")\n",
    "print(f\"Accuracy for explain-then-predict with GPT-3.5 turbo is {pairs_accuracy_explain_pred:.3f} with p-value: {p_3:.3f}\")"
   ],
   "id": "b37d46239074edb"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting GPT3.5 Responses: 100%|██████████| 1000/1000 [43:23<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_explanation_for_finetuning(prompts, truths):\n",
    "    responses = []\n",
    "    for prompt, truth in tqdm(zip(prompts, truths), \n",
    "                              desc=\"Getting GPT3.5 Responses: \", \n",
    "                              total=len(prompts)):\n",
    "        if truth == \"malleable\":\n",
    "            insert = \"We know that he/she did get persuaded by some commentators. How might his/her speeching style and lexical features suggest he/she is malleable to persuasion?\"\n",
    "            \n",
    "        elif truth == \"resistant\":\n",
    "            insert = \"We know that he/she never get persuaded by others. How might his/her speeching style and lexical features suggest he/she is resistant to persuasion?\"\n",
    "            \n",
    "        instruction = f\"You're a semantic analyst. Now I will show you a person's opinion statement, who publicly announced his/her argument and encouraged other people to challenge it. {insert} Very briefly explain your analysis with no more than 2 paragraphs.\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=[\n",
    "                          {\"role\":\"system\", \"content\":instruction},\n",
    "                          prompt['body']['messages'][1]\n",
    "                      ],\n",
    "                    max_completion_tokens=prompt['body']['max_tokens']\n",
    "                    )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {prompt['custom_id']}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses\n",
    "\n",
    "\n",
    "\n",
    "explanation_prompts = load_prompts(\"finetune_llama3_1/op_train.jsonl\")\n",
    "truths = [line[\"output\"] for line in explanation_prompts]\n",
    "train_prompts = load_prompts(\"op_train_prompts.jsonl\")\n",
    "explanations = get_explanation_for_finetuning(train_prompts, truths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T18:33:55.439975Z",
     "start_time": "2024-09-29T17:50:32.114500Z"
    }
   },
   "id": "69b9ebf9fb986218"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def create_jsonl(explanations, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for line in explanations:\n",
    "            content = line.choices[0].message.content\n",
    "            json.dump(content, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "file_path = 'finetune_llama3_1/op_gpt_explanations.jsonl'\n",
    "create_jsonl(explanations, file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T18:33:55.510063Z",
     "start_time": "2024-09-29T18:33:55.463613Z"
    }
   },
   "id": "fb7f443409af0693"
  },
  {
   "cell_type": "code",
   "source": "len(pairs_responses_direct)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T17:59:29.199485Z",
     "start_time": "2024-09-30T17:59:29.196682Z"
    }
   },
   "id": "1958a4409d175fca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ffb8b460bfd83265"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
