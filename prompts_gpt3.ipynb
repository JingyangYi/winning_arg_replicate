{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key='sk-8r8dlxIAf8QZ_Veq3Nl8DEyGrFulMTq0yXDQ1_tqb-T3BlbkFJpCTT3cD4ZluM66BCut-bDh6CVHsp-PgslMUzaEcJgA')\n",
    "\n",
    "# Load prompts from a JSONL file\n",
    "def load_prompts(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "prompts = load_prompts('op_test_prompts.jsonl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:18:42.291793Z",
     "start_time": "2024-09-29T17:18:42.126126Z"
    }
   },
   "id": "f02ac414ef772fff"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# Function to send requests and gather responses\n",
    "def get_responses(prompts, instruction):\n",
    "    responses = []\n",
    "    for prompt in tqdm(prompts, desc=\"Getting GPT3.5 Responses: \"):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=[\n",
    "                          {\"role\":\"system\", \"content\":instruction},\n",
    "                          prompt['body']['messages'][1]\n",
    "                      ],\n",
    "                    max_completion_tokens=prompt['body']['max_tokens']\n",
    "                    )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {prompt['custom_id']}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T16:56:22.255067Z",
     "start_time": "2024-09-29T16:56:22.244406Z"
    }
   },
   "id": "69a03815dbdf6e2a"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "directly_predict = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? Answer with 'malleable' or 'resistant'.\"\n",
    "predict_then_explain = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? Answer with 'malleable' or 'resistant' and explain your answer. Response with the following format: Prediction: resistant/malleable \\n Explanation: briefly explain here.\"\n",
    "explain_then_predict = \"You're a semantic analyst. Now I will show you a person's opinion statement. We know that the person publicly announced his/her argument and encouraged other people to challenge it. Judging from the speech style and lexical features, do you think he/she is resistant or malleable to persuasion? First briefly explain your analysis and then give your answer with resistant/malleable. Response with the following format: Explanation: briefly explain here. \\n  Prediction: resistant/malleable\"\n",
    "\n",
    "responses_direct = get_responses(prompts, directly_predict)\n",
    "responses_pred_explain = get_responses(prompts, predict_then_explain)\n",
    "responses_explain_pred = get_responses(prompts, explain_then_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:07:02.542363Z",
     "start_time": "2024-09-29T16:56:24.373934Z"
    }
   },
   "id": "b85ddc576407e332"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for direct prediction with GPT-3.5 turbo is 0.525.\n",
      "Accuracy for predict-then-explain with GPT-3.5 turbo is 0.555 with p-value: 0.547\n",
      "Accuracy for explain-then-predict with GPT-3.5 turbo is 0.550 with p-value: 0.616\n"
     ]
    }
   ],
   "source": [
    "def prediction_accuracy(responses, file_path, method=\"direct\"):\n",
    "    gpt_predictions = []\n",
    "    if method == \"direct\":\n",
    "        for res in responses:\n",
    "            gpt_predictions.append(res.choices[0].message.content.lower())\n",
    "    else:\n",
    "        for res in responses:\n",
    "            pred = res.choices[0].message.content.lower()\n",
    "            start_point = pred.find(\"prediction: \") + len(\"prediction: \")\n",
    "            # the strings 'malleable' and 'resistant' happen to have the same length of 9\n",
    "            if method == \"predict_then_explain\":\n",
    "                pred = pred[start_point: start_point + 9]\n",
    "            elif method == \"explain_then_predict\":\n",
    "                pred = pred[start_point: start_point + 9]\n",
    "            gpt_predictions.append(pred)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        truths = [json.loads(line)[\"output\"] for line in f]\n",
    "    scores = [1 if gpt_predictions[j] == truths[j] else 0 for j in range(len(truths))]\n",
    "    return sum(scores) / len(truths)\n",
    "\n",
    "def z_test(n_sample, accuracy_1, accuracy_2):\n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    pooled = (accuracy_1 * n_sample + accuracy_2 * n_sample) / (n_sample * 2)\n",
    "    z_score = (accuracy_1 - accuracy_2) / np.sqrt(pooled * (1 - pooled) * 2 / n_sample)\n",
    "    p_value = norm.sf(abs(z_score)) * 2\n",
    "    return p_value\n",
    "\n",
    "file_path = \"finetune_llama3_1/op_test.jsonl\"\n",
    "n_sample = len(responses_direct)\n",
    "\n",
    "accuracy_direct = prediction_accuracy(responses_direct, file_path)\n",
    "accuracy_pred_explain = prediction_accuracy(responses_pred_explain, \n",
    "                                            file_path, \n",
    "                                            method=\"predict_then_explain\")\n",
    "accuracy_explain_pred = prediction_accuracy(responses_explain_pred, \n",
    "                                            file_path, \n",
    "                                            method=\"explain_then_predict\")\n",
    "p_2 = z_test(n_sample, accuracy_direct, accuracy_pred_explain)\n",
    "p_3 = z_test(n_sample, accuracy_direct, accuracy_explain_pred)\n",
    "\n",
    "print(f\"Accuracy for direct prediction with GPT-3.5 turbo is {accuracy_direct:.3f}.\")\n",
    "print(f\"Accuracy for predict-then-explain with GPT-3.5 turbo is {accuracy_pred_explain:.3f} with p-value: {p_2:.3f}\")\n",
    "print(f\"Accuracy for explain-then-predict with GPT-3.5 turbo is {accuracy_explain_pred:.3f} with p-value: {p_3:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T18:39:44.962224Z",
     "start_time": "2024-09-29T18:39:44.946784Z"
    }
   },
   "id": "8daeb90383375fbc"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting GPT3.5 Responses: 100%|██████████| 1000/1000 [43:23<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_explanation_for_finetuning(prompts, truths):\n",
    "    responses = []\n",
    "    for prompt, truth in tqdm(zip(prompts, truths), \n",
    "                              desc=\"Getting GPT3.5 Responses: \", \n",
    "                              total=len(prompts)):\n",
    "        if truth == \"malleable\":\n",
    "            insert = \"We know that he/she did get persuaded by some commentators. How might his/her speeching style and lexical features suggest he/she is malleable to persuasion?\"\n",
    "            \n",
    "        elif truth == \"resistant\":\n",
    "            insert = \"We know that he/she never get persuaded by others. How might his/her speeching style and lexical features suggest he/she is resistant to persuasion?\"\n",
    "            \n",
    "        instruction = f\"You're a semantic analyst. Now I will show you a person's opinion statement, who publicly announced his/her argument and encouraged other people to challenge it. {insert} Very briefly explain your analysis with no more than 2 paragraphs.\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=[\n",
    "                          {\"role\":\"system\", \"content\":instruction},\n",
    "                          prompt['body']['messages'][1]\n",
    "                      ],\n",
    "                    max_completion_tokens=prompt['body']['max_tokens']\n",
    "                    )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {prompt['custom_id']}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses\n",
    "\n",
    "\n",
    "\n",
    "explanation_prompts = load_prompts(\"finetune_llama3_1/op_train.jsonl\")\n",
    "truths = [line[\"output\"] for line in explanation_prompts]\n",
    "train_prompts = load_prompts(\"op_train_prompts.jsonl\")\n",
    "explanations = get_explanation_for_finetuning(train_prompts, truths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T18:33:55.439975Z",
     "start_time": "2024-09-29T17:50:32.114500Z"
    }
   },
   "id": "69b9ebf9fb986218"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def create_jsonl(explanations, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for line in explanations:\n",
    "            content = line.choices[0].message.content\n",
    "            json.dump(content, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "file_path = 'finetune_llama3_1/gpt_explanations.jsonl'\n",
    "create_jsonl(explanations, file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T18:33:55.510063Z",
     "start_time": "2024-09-29T18:33:55.463613Z"
    }
   },
   "id": "fb7f443409af0693"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1958a4409d175fca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
