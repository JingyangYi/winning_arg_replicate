Project 3: Attempts to Prompt and Finetune LLMs as Semantic Classifiers
=====
*This is a report to the UChicago Human+AI lab Trial Project 3: Project 3: Persuasive argument prediction*

## Key Findings:
In this project, I tried various ways to prompt and finetune LLMs (mainly GPT 3.5 and Llama 3.1-8B) to make them achieve better performance on semantic classification tasks with data set from (Tan etal, 16a). Specifically, two tasks are involved: 1) Persuasion prediction: judging which reply is more persuasive to the original poster. 2) Malleability prediction: judging open-mindedness from the claimer's opinion statement. I found that these tasks can be very hard for LLMs. When tested on a balanced dataset with 200 samples, baseline performances of GPT 3.5 or Llama 3.1 are both around 50% of accuracy. However, I found the following methods can improve the predictions performances. 1) Applying a two stages method to prompt the LLM, which can increase the performances largely. 2) Adding prior persuasion knowledge to the prompts. 3) Finetuning LLMs with correct categories and explanations. Besides, I find that prompting GPT 3.5 with explain-then-predict and predict-then-explain instructions merely achieved mixed performances and there isn't much difference between the explain-then-predict and predict-then-explain. 

## Setups
I dealt with 2 LLMs in this small project, GPT 3.5 and Llama 3.1-8B. For GPT 3.5, I simply applied OpenAI's api and did all the prompts. For Llama 3.1-8B, I used [Unsloth](https://github.com/unslothai/unsloth?tab=readme-ov-file) to finetune and make inferences locally. This package can significantly reduce the memory usage and accelerate the finetuning so that I can run it on my PC. For training, I randomly selected 1000 samples as a balanced training set and another 200 samples as a balanced test set. Note that for persuasion task dataset, I arranged the order of the positive and negative replies randomly in a prompt so that when finetuning models it won't just learn to answer a fixed first or second.

## Discussions on My Findings:

- *I only discuss the behaviors of models that involved in this project, more advanced model may perform better.* 
- *Also note that the accuracy rates of each method vary at each implementation since there are randomness during training. That's why I can't give an exact accuracy but discuss only improvements compared to baseline models.*
1. **For tasks like semantic classification, LLMs can only perform at chance level.** Firstly, LLMs do not have a consistent judging criteria for these tasks. Take GPT 3.5 as an example, when prompted at default temperature for multiple times, it tends to give different predictions to the same prompts content. However, if set temperature to a lower level, it instead gives an unchanging prediction to different prompts. These behaviors indicate that it doesn't have a coherent knowledge or capability to make semantic classifications. Secondly, for LLMs, judging which reply is more persuasive or telling if a person is open-minded means compressing information from lengthy paragraphs into one word of persuasiveness or malleability. This compression may increasingly lose accuracy when the paragraphs grow longer. [example here] The average length of original posts + two replies is 3896 words.
2. **Adding knowledge about persuasion to the prompts can improve performances.** I extract these knowledge from the paper (Tan etal, 16a). The knowledge looks like this: *"Hint: 1. Reply Length: Longer replies tend to be more persuasive, as they can convey more information and elaborate on points effectively. 2. Language Dissimilarity....."*, and they are added to the end of each prompt. Intuitively, adding knowledge in prompting can be helpful for the predictions and the level of improvement depends on the quality of the knowledge and LLMs' inference capability. 
3. **Finetuned models generally performs better, but only moderately.** I used two ways to finetune Llama 3.1 8B. Firstly, I solely use the truth tags as training targets. For persuasion prediction they are just "first" or "second" and for malleability prediction they are "resistant" or "malleable". In second method, I added explanations from GPT 3.5 in training targets along with truth tags. For example, for a pair of original post and replies, if the first reply successfully persuaded the original poster, I would ask GPT 3.5 why the first reply is more persuasive and collect the answers. Then, use truth tags and explanations as Llama training targets. As expected, for prediction accuracy, zero-shot < finetuned with tags < finetuned with tags plus explanations. Finetuning with tags improves accuracy by about 5% on both tasks, while finetuning with tags plus explanations further improves by about 3% on both tasks.  
